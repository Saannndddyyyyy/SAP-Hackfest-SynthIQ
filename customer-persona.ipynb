{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":2549419,"datasetId":1546318,"databundleVersionId":2592425}],"dockerImageVersionId":31286,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-27T11:33:20.304068Z","iopub.execute_input":"2026-02-27T11:33:20.304563Z","iopub.status.idle":"2026-02-27T11:33:20.321405Z","shell.execute_reply.started":"2026-02-27T11:33:20.304534Z","shell.execute_reply":"2026-02-27T11:33:20.320451Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/customer-personality-analysis/marketing_campaign.csv\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom ctgan import CTGAN\n\n# ─────────────────────────────────────────────────────────────\n# STEP 1: DATA LOADING & PREPROCESSING\n# ─────────────────────────────────────────────────────────────\n\ndf = pd.read_csv(\"/kaggle/input/customer-personality-analysis/marketing_campaign.csv\"\n    ,\n    sep=\"\\t\"\n)\n\nprint(f\"Original shape: {df.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T11:33:20.322948Z","iopub.execute_input":"2026-02-27T11:33:20.323173Z","iopub.status.idle":"2026-02-27T11:33:20.342469Z","shell.execute_reply.started":"2026-02-27T11:33:20.323154Z","shell.execute_reply":"2026-02-27T11:33:20.341696Z"}},"outputs":[{"name":"stdout","text":"Original shape: (2240, 29)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":" !pip install ctgan -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T11:33:20.343588Z","iopub.execute_input":"2026-02-27T11:33:20.343896Z","iopub.status.idle":"2026-02-27T11:33:23.912057Z","shell.execute_reply.started":"2026-02-27T11:33:20.343875Z","shell.execute_reply":"2026-02-27T11:33:23.910783Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"pip install --upgrade ctgan","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T11:33:23.914057Z","iopub.execute_input":"2026-02-27T11:33:23.914437Z","iopub.status.idle":"2026-02-27T11:33:27.718990Z","shell.execute_reply.started":"2026-02-27T11:33:23.914403Z","shell.execute_reply":"2026-02-27T11:33:27.717969Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: ctgan in /usr/local/lib/python3.12/dist-packages (0.12.1)\nRequirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from ctgan) (2.0.2)\nRequirement already satisfied: pandas>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from ctgan) (2.3.3)\nRequirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from ctgan) (2.9.0+cpu)\nRequirement already satisfied: tqdm>=4.29 in /usr/local/lib/python3.12/dist-packages (from ctgan) (4.67.1)\nRequirement already satisfied: rdt>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from ctgan) (1.20.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.1.1->ctgan) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.1.1->ctgan) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.1.1->ctgan) (2025.3)\nRequirement already satisfied: scipy>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from rdt>=1.14.0->ctgan) (1.16.3)\nRequirement already satisfied: scikit-learn>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from rdt>=1.14.0->ctgan) (1.6.1)\nRequirement already satisfied: Faker!=37.11.0,>=17 in /usr/local/lib/python3.12/dist-packages (from rdt>=1.14.0->ctgan) (40.5.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (3.20.3)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (4.15.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (1.14.0)\nRequirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (3.6.1)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (3.1.6)\nRequirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan) (2025.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.1.1->ctgan) (1.17.0)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.1->rdt>=1.14.0->ctgan) (1.5.3)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.1->rdt>=1.14.0->ctgan) (3.6.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.3.0->ctgan) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.3.0->ctgan) (3.0.3)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"Step 1- Data preprocessing","metadata":{}},{"cell_type":"code","source":"# ── 1a. Clean Income ─────────────────────────────────────────\ndf[\"Income\"] = df[\"Income\"].fillna(df[\"Income\"].median())\n\n# ── 1b. Feature Engineering ──────────────────────────────────\nCURRENT_YEAR = 2024\ndf[\"Age\"] = CURRENT_YEAR - df[\"Year_Birth\"]\n\nmnt_cols = [c for c in df.columns if c.startswith(\"Mnt\")]\ndf[\"TotalSpent\"] = df[mnt_cols].sum(axis=1)\n\n# ── 1c. Simplify Education (→ 3 categories) ──────────────────\nedu_map = {\n    \"Basic\"      : \"Low\",\n    \"2n Cycle\"   : \"Mid\",\n    \"Graduation\" : \"Mid\",\n    \"Master\"     : \"High\",\n    \"PhD\"        : \"High\",\n}\ndf[\"Education\"] = df[\"Education\"].map(edu_map).fillna(\"Mid\")\n\n# ── 1d. Simplify Marital Status (→ 4 categories) ─────────────\nmarital_map = {\n    \"Single\"   : \"Single\",\n    \"Divorced\" : \"Single\",\n    \"Widow\"    : \"Single\",\n    \"Alone\"    : \"Single\",\n    \"Absurd\"   : \"Single\",\n    \"YOLO\"     : \"Single\",\n    \"Married\"  : \"Partnered\",\n    \"Together\" : \"Partnered\",\n}\ndf[\"Marital_Status\"] = df[\"Marital_Status\"].map(marital_map).fillna(\"Single\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T11:33:27.721974Z","iopub.execute_input":"2026-02-27T11:33:27.722250Z","iopub.status.idle":"2026-02-27T11:33:27.740095Z","shell.execute_reply.started":"2026-02-27T11:33:27.722219Z","shell.execute_reply":"2026-02-27T11:33:27.738829Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# ── 1e. Select & clean modelling columns ─────────────────────\nCONTINUOUS_COLS = [\"Income\", \"Age\", \"TotalSpent\", \"Recency\",\n                   \"NumWebPurchases\", \"NumStorePurchases\", \"NumCatalogPurchases\"]\nDISCRETE_COLS   = [\"Education\", \"Marital_Status\", \"Kidhome\", \"Teenhome\",\n                   \"AcceptedCmp1\", \"AcceptedCmp2\", \"AcceptedCmp3\",\n                   \"AcceptedCmp4\", \"AcceptedCmp5\", \"Response\"]\n\nALL_COLS = CONTINUOUS_COLS + DISCRETE_COLS\ndf_clean = df[ALL_COLS].dropna().reset_index(drop=True)\n\n# Cast discrete columns to string so CTGAN treats them categorically\nfor col in DISCRETE_COLS:\n    df_clean[col] = df_clean[col].astype(str)\n\nprint(f\"Cleaned shape  : {df_clean.shape}\")\nprint(f\"Discrete cols  : {DISCRETE_COLS}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T11:33:27.741631Z","iopub.execute_input":"2026-02-27T11:33:27.742019Z","iopub.status.idle":"2026-02-27T11:33:27.772808Z","shell.execute_reply.started":"2026-02-27T11:33:27.741980Z","shell.execute_reply":"2026-02-27T11:33:27.771775Z"}},"outputs":[{"name":"stdout","text":"Cleaned shape  : (2240, 17)\nDiscrete cols  : ['Education', 'Marital_Status', 'Kidhome', 'Teenhome', 'AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Response']\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":" ─────────────────────────────────────────────────────────────\n# STEP 2: CTGAN — TRAIN & GENERATE SYNTHETIC DATA\n# ─────────────────────────────────────────────────────────────\n","metadata":{}},{"cell_type":"code","source":"\nprint(\"\\n[CTGAN] Training synthesizer for 100 epochs …\")\n\nsynthesizer = CTGAN(\n    epochs=100,\n    batch_size=500,\n    verbose=True,\n)\nsynthesizer.fit(df_clean, discrete_columns=DISCRETE_COLS)\n\nprint(\"\\n[CTGAN] Generating 1,500 synthetic rows …\")\nsynthetic_df = synthesizer.sample(1500)\n\n# ── Post-generation sanity fixes ─────────────────────────────\n# Enforce non-negative values on numeric columns that must be ≥ 0\nnon_neg_cols = [\"Income\", \"Age\", \"TotalSpent\", \"Recency\",\n                \"NumWebPurchases\", \"NumStorePurchases\", \"NumCatalogPurchases\"]\nfor col in non_neg_cols:\n    synthetic_df[col] = synthetic_df[col].clip(lower=0)\n\n# Cap Age to a realistic range\nsynthetic_df[\"Age\"] = synthetic_df[\"Age\"].clip(18, 100)\n\nprint(f\"Synthetic data shape: {synthetic_df.shape}\")\nprint(\"\\nSynthetic sample:\")\nprint(synthetic_df.head(3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T11:33:27.774109Z","iopub.execute_input":"2026-02-27T11:33:27.774474Z","iopub.status.idle":"2026-02-27T11:33:58.838835Z","shell.execute_reply.started":"2026-02-27T11:33:27.774452Z","shell.execute_reply":"2026-02-27T11:33:58.838078Z"}},"outputs":[{"name":"stdout","text":"\n[CTGAN] Training synthesizer for 100 epochs …\n","output_type":"stream"},{"name":"stderr","text":"Gen. (-01.26) | Discrim. (-00.04): 100%|██████████| 100/100 [00:23<00:00,  4.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n[CTGAN] Generating 1,500 synthetic rows …\nSynthetic data shape: (1500, 17)\n\nSynthetic sample:\n         Income  Age  TotalSpent  Recency  NumWebPurchases  NumStorePurchases  \\\n0  93209.827803   63        1576        0                1                  5   \n1  25955.680980   74           0       38                2                 11   \n2  48841.769087   42         591       76                2                  3   \n\n   NumCatalogPurchases Education Marital_Status Kidhome Teenhome AcceptedCmp1  \\\n0                    3      High      Partnered       1        0            0   \n1                   11       Mid      Partnered       0        1            0   \n2                    0       Mid      Partnered       0        1            0   \n\n  AcceptedCmp2 AcceptedCmp3 AcceptedCmp4 AcceptedCmp5 Response  \n0            0            0            0            1        1  \n1            0            0            0            0        0  \n2            0            0            0            0        1  \n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"# ─────────────────────────────────────────────────────────────\n# STEP 3: COMBINE & CLUSTER (K-MEANS)\n# ─────────────────────────────────────────────────────────────","metadata":{}},{"cell_type":"code","source":"# ── 3a. Combine real + synthetic ─────────────────────────────\ndf_clean[\"source\"]    = \"real\"\nsynthetic_df[\"source\"] = \"synthetic\"\ncombined = pd.concat([df_clean, synthetic_df], ignore_index=True)\n\nprint(f\"\\nCombined dataset shape: {combined.shape}\")\n\n# ── 3b. Scale continuous features ────────────────────────────\nscaler     = StandardScaler()\nX_scaled   = scaler.fit_transform(combined[CONTINUOUS_COLS])\n\n# ── 3c. Elbow Method (print inertias, pick best k) ───────────\nprint(\"\\n[Elbow Method] Inertia by k:\")\ninertias = {}\nfor k in range(2, 9):\n    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n    km.fit(X_scaled)\n    inertias[k] = km.inertia_\n    print(f\"  k={k}  →  inertia={km.inertia_:,.0f}\")\n\n# Automatically pick k with the largest drop (elbow heuristic)\ndrops = {k: inertias[k-1] - inertias[k] for k in range(3, 9)}\noptimal_k = max(drops, key=drops.get)          # k that gives biggest gain\noptimal_k = max(4, min(optimal_k, 5))          # constrain to 4-5 per requirement\nprint(f\"\\n→ Optimal k selected: {optimal_k}\")\n\n# ── 3d. Final KMeans model ───────────────────────────────────\nkmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\ncombined[\"Cluster\"] = kmeans.fit_predict(X_scaled)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T11:33:58.840077Z","iopub.execute_input":"2026-02-27T11:33:58.840962Z","iopub.status.idle":"2026-02-27T11:33:59.417955Z","shell.execute_reply.started":"2026-02-27T11:33:58.840929Z","shell.execute_reply":"2026-02-27T11:33:59.416884Z"}},"outputs":[{"name":"stdout","text":"\nCombined dataset shape: (3740, 18)\n\n[Elbow Method] Inertia by k:\n  k=2  →  inertia=19,914\n  k=3  →  inertia=17,561\n  k=4  →  inertia=16,253\n  k=5  →  inertia=15,312\n  k=6  →  inertia=14,432\n  k=7  →  inertia=13,753\n  k=8  →  inertia=13,167\n\n→ Optimal k selected: 4\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"# ─────────────────────────────────────────────────────────────\n# STEP 4: CLUSTER PROFILES\n# ─────────────────────────────────────────────────────────────\n","metadata":{}},{"cell_type":"code","source":"profile_cols = [\"Income\", \"TotalSpent\", \"Age\",\n                \"Recency\", \"NumWebPurchases\", \"NumStorePurchases\"]\n\ncluster_profiles = (\n    combined\n    .groupby(\"Cluster\")[profile_cols]\n    .mean()\n    .round(2)\n)\n\n# Add cluster size\ncluster_profiles[\"Count\"]        = combined.groupby(\"Cluster\")[\"Income\"].count()\ncluster_profiles[\"SyntheticPct\"] = (\n    combined[combined[\"source\"] == \"synthetic\"]\n    .groupby(\"Cluster\")[\"Income\"].count()\n    .div(cluster_profiles[\"Count\"])\n    .mul(100)\n    .round(1)\n)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"CLUSTER PROFILES — Mean Values\")\nprint(\"=\"*70)\nprint(cluster_profiles.to_string())\nprint(\"=\"*70)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T11:33:59.419190Z","iopub.execute_input":"2026-02-27T11:33:59.419565Z","iopub.status.idle":"2026-02-27T11:33:59.437190Z","shell.execute_reply.started":"2026-02-27T11:33:59.419543Z","shell.execute_reply":"2026-02-27T11:33:59.436346Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nCLUSTER PROFILES — Mean Values\n======================================================================\n           Income  TotalSpent    Age  Recency  NumWebPurchases  NumStorePurchases  Count  SyntheticPct\nCluster                                                                                               \n0        73614.74     1350.65  56.00    47.37             5.51               8.71    841          23.5\n1        64922.77      514.98  60.22    71.50             5.64               6.14    788          53.4\n2        33673.37      115.59  51.63    47.42             2.03               3.12   1063          11.9\n3        68801.37      509.44  58.83    15.32             6.04               5.68   1048          72.0\n======================================================================\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# Optional: Label clusters by spend tier for readability\nspend_order = cluster_profiles[\"TotalSpent\"].rank().astype(int)\nlabel_map   = {idx: f\"Cluster {i} (Tier {rank})\"\n               for i, (idx, rank) in enumerate(spend_order.items())}\ncluster_profiles.index = cluster_profiles.index.map(label_map)\n\nprint(\"\\nLabelled profiles:\")\nprint(cluster_profiles[[\"Income\", \"TotalSpent\", \"Age\", \"Count\"]].to_string())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T11:33:59.438416Z","iopub.execute_input":"2026-02-27T11:33:59.438715Z","iopub.status.idle":"2026-02-27T11:33:59.451464Z","shell.execute_reply.started":"2026-02-27T11:33:59.438695Z","shell.execute_reply":"2026-02-27T11:33:59.450490Z"}},"outputs":[{"name":"stdout","text":"\nLabelled profiles:\n                      Income  TotalSpent    Age  Count\nCluster                                               \nCluster 0 (Tier 4)  73614.74     1350.65  56.00    841\nCluster 1 (Tier 3)  64922.77      514.98  60.22    788\nCluster 2 (Tier 1)  33673.37      115.59  51.63   1063\nCluster 3 (Tier 2)  68801.37      509.44  58.83   1048\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"# ─────────────────────────────────────────────────────────────\n# STEP 5 ─ Drift Engine (February 2026 Baseline)\n# ─────────────────────────────────────────────────────────────","metadata":{}},{"cell_type":"code","source":"\nMARKET_SIGNALS = {\n    \"inflation\"      : 2.75,   # %\n    \"interest_rate\"  : 5.25,   # Repo Rate %\n    \"unemployment\"   : 5.00,   # %\n    \"gdp_growth\"     : 7.80,   # %  (used as a moderating factor)\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T11:33:59.452633Z","iopub.execute_input":"2026-02-27T11:33:59.452961Z","iopub.status.idle":"2026-02-27T11:33:59.464861Z","shell.execute_reply.started":"2026-02-27T11:33:59.452929Z","shell.execute_reply":"2026-02-27T11:33:59.463924Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"CLUSTER_PERSONAS = pd.DataFrame({\n    \"Cluster\"            : [\"C1\", \"C2\", \"C3\", \"C4\", \"C5\"],\n    \"Label\"              : [\n        \"Conservative Low-Income\",\n        \"Young Mid-Spender\",\n        \"Affluent High-Spender\",\n        \"Cautious Saver\",\n        \"Premium Power-Buyer\",\n    ],\n    \"Income\"             : [28_000, 52_000, 88_000, 45_000, 130_000],\n    \"TotalSpent\"         : [  210,     780,   1_850,    430,    2_950],\n    \"Age\"                : [   55,      32,      44,     48,       39],\n    # Behavioural attributes (all on 0-1 scale)\n    \"risk_tolerance\"     : [0.20, 0.55, 0.70, 0.35, 0.85],\n    \"spending_propensity\": [0.25, 0.60, 0.75, 0.40, 0.90],\n    \"default_prob\"       : [0.18, 0.10, 0.05, 0.12, 0.03],\n})\n\n# ─────────────────────────────────────────────────────────────\n# STEP 2B ─ CLUSTER-SPECIFIC SENSITIVITY COEFFICIENTS\n#           α → risk erosion   β → spending erosion\n#           γ → default risk amplification\n# ─────────────────────────────────────────────────────────────\n\nSENSITIVITY = {\n    #  cluster  alpha    beta    gamma\n    \"C1\": dict(alpha=0.030, beta=0.025, gamma=0.020),  # high macro sensitivity\n    \"C2\": dict(alpha=0.020, beta=0.018, gamma=0.015),  # moderate young earner\n    \"C3\": dict(alpha=0.012, beta=0.010, gamma=0.008),  # resilient affluent\n    \"C4\": dict(alpha=0.022, beta=0.020, gamma=0.017),  # cautious, mid-sensitive\n    \"C5\": dict(alpha=0.008, beta=0.007, gamma=0.005),  # near-immune premium\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T11:34:31.851414Z","iopub.execute_input":"2026-02-27T11:34:31.851710Z","iopub.status.idle":"2026-02-27T11:34:31.859320Z","shell.execute_reply.started":"2026-02-27T11:34:31.851689Z","shell.execute_reply":"2026-02-27T11:34:31.858373Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"def calculate_fsi(signals: dict) -> float:\n    \"\"\"\n    Financial Stress Index (FSI)\n    FSI = 0.4×inflation + 0.3×interest_rate + 0.3×unemployment\n\n    GDP growth acts as a natural moderator: if gdp_growth > 6 %\n    (a robust economy) we apply a mild dampening factor so that\n    strong growth partially offsets macro stress.\n    \"\"\"\n    raw_fsi = (\n        0.4 * signals[\"inflation\"] +\n        0.3 * signals[\"interest_rate\"] +\n        0.3 * signals[\"unemployment\"]\n    )\n\n    # GDP moderator: each 1 % above 6 % baseline dampens FSI by 1.5 %\n    gdp_baseline  = 6.0\n    gdp_bonus     = max(0, signals[\"gdp_growth\"] - gdp_baseline)\n    dampening     = 1 - (0.015 * gdp_bonus)          # e.g. 7.8 % → factor ≈ 0.973\n    moderated_fsi = raw_fsi * dampening\n\n    print(f\"  Raw FSI          : {raw_fsi:.4f}\")\n    print(f\"  GDP Dampening    : {dampening:.4f}  (GDP={signals['gdp_growth']}%)\")\n    print(f\"  Moderated FSI    : {moderated_fsi:.4f}\")\n    return moderated_fsi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T11:33:59.481112Z","iopub.execute_input":"2026-02-27T11:33:59.481465Z","iopub.status.idle":"2026-02-27T11:33:59.497987Z","shell.execute_reply.started":"2026-02-27T11:33:59.481433Z","shell.execute_reply":"2026-02-27T11:33:59.497158Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"def apply_drift(persona_row: pd.Series, fsi: float) -> pd.Series:\n    \"\"\"\n    Drift equations\n      risk_tolerance(t+1)      = risk_tolerance(t)      - α × FSI\n      spending_propensity(t+1) = spending_propensity(t) - β × FSI\n      default_prob(t+1)        = default_prob(t)        + γ × FSI\n\n    All outputs are clipped to [0, 1] to stay on a probability scale.\n    \"\"\"\n    cluster = persona_row[\"Cluster\"]\n    coeffs  = SENSITIVITY[cluster]\n\n    drifted = persona_row.copy()\n\n    drifted[\"risk_tolerance\"] = np.clip(\n        persona_row[\"risk_tolerance\"] - coeffs[\"alpha\"] * fsi, 0.0, 1.0\n    )\n    drifted[\"spending_propensity\"] = np.clip(\n        persona_row[\"spending_propensity\"] - coeffs[\"beta\"] * fsi, 0.0, 1.0\n    )\n    drifted[\"default_prob\"] = np.clip(\n        persona_row[\"default_prob\"] + coeffs[\"gamma\"] * fsi, 0.0, 1.0\n    )\n    return drifted","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T11:33:59.500599Z","iopub.execute_input":"2026-02-27T11:33:59.500923Z","iopub.status.idle":"2026-02-27T11:33:59.515400Z","shell.execute_reply.started":"2026-02-27T11:33:59.500902Z","shell.execute_reply":"2026-02-27T11:33:59.514431Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"def predict_purchase_probability(\n    income: float,\n    total_spent: float,\n    risk_tolerance: float,\n    fsi: float,\n    income_max: float = 130_000,\n    spent_max:  float = 2_950,\n) -> float:\n    \"\"\"\n    Composite Purchase Probability Score  (0 – 100)\n\n    Components\n    ──────────\n    • Income Score      → normalised income (higher = more capacity)  weight 25 %\n    • Spend Score       → normalised historical spend (habit proxy)   weight 35 %\n    • Risk Score        → post-drift risk tolerance                   weight 30 %\n    • Stress Penalty    → FSI drags down probability                  weight 10 %\n\n    Final score is sigmoid-smoothed to avoid hard boundaries.\n    \"\"\"\n    income_score = (income / income_max) * 100\n    spend_score  = (total_spent / spent_max) * 100\n    risk_score   = risk_tolerance * 100\n\n    # FSI penalty: normalise FSI (theoretical max ≈ 10) → % penalty\n    fsi_max      = 10.0\n    stress_penalty = (fsi / fsi_max) * 100\n\n    raw_score = (\n        0.25 * income_score +\n        0.35 * spend_score  +\n        0.30 * risk_score   -\n        0.10 * stress_penalty\n    )\n\n    # Sigmoid smoothing: keeps output in (0, 100) with natural taper at extremes\n    def sigmoid(x, midpoint=50, steepness=0.07):\n        return 100 / (1 + np.exp(-steepness * (x - midpoint)))\n\n    smoothed = sigmoid(raw_score)\n    return round(float(np.clip(smoothed, 0, 100)), 2)\n\n\ndef campaign_success_pct(cluster_df: pd.DataFrame, fsi: float) -> pd.Series:\n    \"\"\"Compute per-cluster mean purchase probability after drift.\"\"\"\n    scores = cluster_df.apply(\n        lambda row: predict_purchase_probability(\n            income         = row[\"Income\"],\n            total_spent    = row[\"TotalSpent\"],\n            risk_tolerance = row[\"risk_tolerance\"],\n            fsi            = fsi,\n        ),\n        axis=1,\n    )\n    return scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T11:33:59.516621Z","iopub.execute_input":"2026-02-27T11:33:59.516958Z","iopub.status.idle":"2026-02-27T11:33:59.530199Z","shell.execute_reply.started":"2026-02-27T11:33:59.516925Z","shell.execute_reply":"2026-02-27T11:33:59.529361Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"def run_drift_engine():\n    print(\"=\" * 65)\n    print(\"  DriftLab — Behavioral Drift Engine  |  February 2026\")\n    print(\"=\" * 65)\n\n    # ── 4a. Compute FSI ───────────────────────────────────────\n    print(\"\\n[1] Computing Financial Stress Index (FSI) …\")\n    fsi = calculate_fsi(MARKET_SIGNALS)\n\n    # ── 4b. Apply drift to every cluster ─────────────────────\n    print(\"\\n[2] Applying drift equations to cluster personas …\")\n    before_df = CLUSTER_PERSONAS.copy()\n    after_rows = [apply_drift(row, fsi) for _, row in before_df.iterrows()]\n    after_df  = pd.DataFrame(after_rows).reset_index(drop=True)\n\n    # ── 4c. Compute purchase probabilities ───────────────────\n    print(\"\\n[3] Computing Purchase Probability Scores …\")\n    before_df[\"purchase_prob\"] = campaign_success_pct(before_df, fsi=0)   # no stress\n    after_df[\"purchase_prob\"]  = campaign_success_pct(after_df,  fsi=fsi) # with FSI\n\n    # ── 4d. Compute per-cluster drift deltas ──────────────────\n    DRIFT_COLS = [\"risk_tolerance\", \"spending_propensity\", \"default_prob\", \"purchase_prob\"]\n\n    # ── 4e. Assemble side-by-side report DataFrame ───────────\n    report_rows = []\n    for i, cluster_id in enumerate(before_df[\"Cluster\"]):\n        b = before_df.iloc[i]\n        a = after_df.iloc[i]\n        coeffs = SENSITIVITY[cluster_id]\n\n        report_rows.append({\n            # Identity\n            \"Cluster\"                        : cluster_id,\n            \"Label\"                          : b[\"Label\"],\n            # Financials\n            \"Income\"                         : int(b[\"Income\"]),\n            \"TotalSpent\"                     : int(b[\"TotalSpent\"]),\n            \"Age\"                            : int(b[\"Age\"]),\n            # Sensitivity coefficients\n            \"α (risk)\"                       : coeffs[\"alpha\"],\n            \"β (spend)\"                      : coeffs[\"beta\"],\n            \"γ (default)\"                    : coeffs[\"gamma\"],\n            # BEFORE drift\n            \"RiskTol_Before\"                 : round(b[\"risk_tolerance\"],      4),\n            \"SpendProp_Before\"               : round(b[\"spending_propensity\"], 4),\n            \"DefaultProb_Before\"             : round(b[\"default_prob\"],        4),\n            \"PurchaseProb_Before (%)\"        : round(b[\"purchase_prob\"],       2),\n            # AFTER drift (Feb 2026)\n            \"RiskTol_After\"                  : round(a[\"risk_tolerance\"],      4),\n            \"SpendProp_After\"                : round(a[\"spending_propensity\"], 4),\n            \"DefaultProb_After\"              : round(a[\"default_prob\"],        4),\n            \"PurchaseProb_After (%)\"         : round(a[\"purchase_prob\"],       2),\n            # Deltas\n            \"ΔRiskTol\"                       : round(a[\"risk_tolerance\"]      - b[\"risk_tolerance\"],      4),\n            \"ΔSpendProp\"                     : round(a[\"spending_propensity\"] - b[\"spending_propensity\"], 4),\n            \"ΔDefaultProb\"                   : round(a[\"default_prob\"]        - b[\"default_prob\"],        4),\n            \"ΔPurchaseProb (pp)\"             : round(a[\"purchase_prob\"]       - b[\"purchase_prob\"],       2),\n            # FSI context\n            \"FSI\"                            : round(fsi, 4),\n        })\n\n    report_df = pd.DataFrame(report_rows)\n\n    # ── 4f. Print formatted report ────────────────────────────\n    print(\"\\n\" + \"=\" * 65)\n    print(\"  BEHAVIORAL DRIFT REPORT  |  Before vs After (Feb 2026)\")\n    print(\"=\" * 65)\n\n    # Split wide table into readable blocks\n    identity_cols   = [\"Cluster\", \"Label\", \"Income\", \"TotalSpent\", \"Age\", \"FSI\"]\n    coeff_cols      = [\"Cluster\", \"α (risk)\", \"β (spend)\", \"γ (default)\"]\n    before_cols     = [\"Cluster\", \"RiskTol_Before\", \"SpendProp_Before\",\n                       \"DefaultProb_Before\", \"PurchaseProb_Before (%)\"]\n    after_cols      = [\"Cluster\", \"RiskTol_After\", \"SpendProp_After\",\n                       \"DefaultProb_After\", \"PurchaseProb_After (%)\"]\n    delta_cols      = [\"Cluster\", \"ΔRiskTol\", \"ΔSpendProp\",\n                       \"ΔDefaultProb\", \"ΔPurchaseProb (pp)\"]\n    success_cols    = [\"Cluster\", \"Label\",\n                       \"PurchaseProb_Before (%)\", \"PurchaseProb_After (%)\",\n                       \"ΔPurchaseProb (pp)\"]\n\n    block_headers = [\n        (\"── Cluster Identity & Market Context\", identity_cols),\n        (\"── Sensitivity Coefficients per Cluster\", coeff_cols),\n        (\"── BEFORE Drift Attributes\",  before_cols),\n        (\"── AFTER Drift Attributes (Feb 2026 FSI Applied)\", after_cols),\n        (\"── Drift Deltas (After − Before)\", delta_cols),\n        (\"── Campaign Success Probability Summary\", success_cols),\n    ]\n\n    for title, cols in block_headers:\n        print(f\"\\n{title}\")\n        print(report_df[cols].to_string(index=False))\n\n    print(\"\\n\" + \"=\" * 65)\n    print(\"  INTERPRETATION GUIDE\")\n    print(\"=\" * 65)\n    print(f\"  FSI (Feb 2026)   : {fsi:.4f}  — Moderate Stress Zone\")\n    print(f\"  Market Signals   : Inflation={MARKET_SIGNALS['inflation']}% | \"\n          f\"Rate={MARKET_SIGNALS['interest_rate']}% | \"\n          f\"Unemployment={MARKET_SIGNALS['unemployment']}% | \"\n          f\"GDP={MARKET_SIGNALS['gdp_growth']}%\")\n    print()\n\n    for _, row in report_df.iterrows():\n        direction = \"▲ RESILIENT\" if row[\"ΔPurchaseProb (pp)\"] > -3 else \"▼ AT RISK\"\n        print(f\"  {row['Cluster']} | {row['Label']:<28} | \"\n              f\"Success: {row['PurchaseProb_After (%)']:>5.2f}%  [{direction}]  \"\n              f\"Δ={row['ΔPurchaseProb (pp)']:+.2f}pp\")\n\n    print(\"=\" * 65)\n\n    return report_df, fsi\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T11:33:59.531534Z","iopub.execute_input":"2026-02-27T11:33:59.531850Z","iopub.status.idle":"2026-02-27T11:33:59.556789Z","shell.execute_reply.started":"2026-02-27T11:33:59.531820Z","shell.execute_reply":"2026-02-27T11:33:59.555700Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    report_df, fsi = run_drift_engine()\n\n    # Save to CSV for downstream use\n    report_df.to_csv(\"driftlab_report_feb2026.csv\", index=False)\n    print(\"\\n✔  Report saved to  driftlab_report_feb2026.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T11:34:59.412890Z","iopub.execute_input":"2026-02-27T11:34:59.413836Z","iopub.status.idle":"2026-02-27T11:34:59.443504Z","shell.execute_reply.started":"2026-02-27T11:34:59.413807Z","shell.execute_reply":"2026-02-27T11:34:59.442577Z"}},"outputs":[{"name":"stdout","text":"=================================================================\n  DriftLab — Behavioral Drift Engine  |  February 2026\n=================================================================\n\n[1] Computing Financial Stress Index (FSI) …\n  Raw FSI          : 4.1750\n  GDP Dampening    : 0.9730  (GDP=7.8%)\n  Moderated FSI    : 4.0623\n\n[2] Applying drift equations to cluster personas …\n\n[3] Computing Purchase Probability Scores …\n\n=================================================================\n  BEHAVIORAL DRIFT REPORT  |  Before vs After (Feb 2026)\n=================================================================\n\n── Cluster Identity & Market Context\nCluster                   Label  Income  TotalSpent  Age    FSI\n     C1 Conservative Low-Income   28000         210   55 4.0623\n     C2       Young Mid-Spender   52000         780   32 4.0623\n     C3   Affluent High-Spender   88000        1850   44 4.0623\n     C4          Cautious Saver   45000         430   48 4.0623\n     C5     Premium Power-Buyer  130000        2950   39 4.0623\n\n── Sensitivity Coefficients per Cluster\nCluster  α (risk)  β (spend)  γ (default)\n     C1     0.030      0.025        0.020\n     C2     0.020      0.018        0.015\n     C3     0.012      0.010        0.008\n     C4     0.022      0.020        0.017\n     C5     0.008      0.007        0.005\n\n── BEFORE Drift Attributes\nCluster  RiskTol_Before  SpendProp_Before  DefaultProb_Before  PurchaseProb_Before (%)\n     C1            0.20              0.25                0.18                     7.39\n     C2            0.55              0.60                0.10                    26.95\n     C3            0.70              0.75                0.05                    66.62\n     C4            0.35              0.40                0.12                    14.16\n     C5            0.85              0.90                0.03                    92.31\n\n── AFTER Drift Attributes (Feb 2026 FSI Applied)\nCluster  RiskTol_After  SpendProp_After  DefaultProb_After  PurchaseProb_After (%)\n     C1         0.0781           0.1484             0.2612                    4.44\n     C2         0.4688           0.5269             0.1609                   18.97\n     C3         0.6513           0.7094             0.0825                   57.55\n     C4         0.2606           0.3188             0.1891                    9.33\n     C5         0.8175           0.8716             0.0503                   89.40\n\n── Drift Deltas (After − Before)\nCluster  ΔRiskTol  ΔSpendProp  ΔDefaultProb  ΔPurchaseProb (pp)\n     C1   -0.1219     -0.1016        0.0812               -2.95\n     C2   -0.0812     -0.0731        0.0609               -7.98\n     C3   -0.0487     -0.0406        0.0325               -9.07\n     C4   -0.0894     -0.0812        0.0691               -4.83\n     C5   -0.0325     -0.0284        0.0203               -2.91\n\n── Campaign Success Probability Summary\nCluster                   Label  PurchaseProb_Before (%)  PurchaseProb_After (%)  ΔPurchaseProb (pp)\n     C1 Conservative Low-Income                     7.39                    4.44               -2.95\n     C2       Young Mid-Spender                    26.95                   18.97               -7.98\n     C3   Affluent High-Spender                    66.62                   57.55               -9.07\n     C4          Cautious Saver                    14.16                    9.33               -4.83\n     C5     Premium Power-Buyer                    92.31                   89.40               -2.91\n\n=================================================================\n  INTERPRETATION GUIDE\n=================================================================\n  FSI (Feb 2026)   : 4.0623  — Moderate Stress Zone\n  Market Signals   : Inflation=2.75% | Rate=5.25% | Unemployment=5.0% | GDP=7.8%\n\n  C1 | Conservative Low-Income      | Success:  4.44%  [▲ RESILIENT]  Δ=-2.95pp\n  C2 | Young Mid-Spender            | Success: 18.97%  [▼ AT RISK]  Δ=-7.98pp\n  C3 | Affluent High-Spender        | Success: 57.55%  [▼ AT RISK]  Δ=-9.07pp\n  C4 | Cautious Saver               | Success:  9.33%  [▼ AT RISK]  Δ=-4.83pp\n  C5 | Premium Power-Buyer          | Success: 89.40%  [▲ RESILIENT]  Δ=-2.91pp\n=================================================================\n\n✔  Report saved to  driftlab_report_feb2026.csv\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}